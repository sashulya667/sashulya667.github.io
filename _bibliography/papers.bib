---
---

@article{kumar-etal-2024-confidence,
  abbr={ACL - Confidence},
  title={Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models},
  author={Kumar, A. and Morabito, R. and Umbet, S. and Kabbara, J. and Emami, A.},
  abstract={As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models. We introduce the concept of Confidence-Probability Alignment, that connects an LLM‚Äôs internal confidence, quantified by token probabilities, to the confidence conveyed in the model‚Äôs response when explicitly asked about its certainty. Using various datasets and prompting techniques that encourage model introspection, we probe the alignment between models‚Äô internal and expressed confidence. These techniques encompass using structured evaluation scales to rate confidence, including answer options when prompting, and eliciting the model‚Äôs confidence level for outputs it does not recognize as its own. Notably, among the models analyzed, OpenAI‚Äôs GPT-4 showed the strongest confidence-probability alignment, with an average Spearman‚Äôs  ÃÇùúå of 0.42, across a wide range of tasks. Our work contributes to the ongoing efforts to facilitate risk assessment in the application of LLMs and to further our understanding of model trustworthiness.},
  year={2024},
  month={August},
  doi={10.18653/v1/2024.acl-long.20},
  selected=true,
  url={https://aclanthology.org/2024.acl-long.20/},
  preview={acl1_animated.gif},
}


@book{kazbench,
  bibtex_show={true},
  title={KazBench-KK: A Cultural-Knowledge Benchmark for Kazakh},
  abstract={We introduce KazBench-KK, a comprehensive 7,111-question multiple-choice benchmark designed to assess large language models‚Äô understanding of culturally grounded Kazakh knowledge. By combining expert- curated topics with LLM-assisted web mining, we create a diverse dataset spanning 17 culturally salient domains, including pastoral traditions, social hierarchies, and contemporary politics. Beyond evaluation, KazBench-KK serves as a practical tool for field linguists, enabling rapid lexical elicitation, glossing, and topic prioritization. Our benchmarking of various open-source LLMs reveals that reinforcement-tuned models outperform others, but smaller, domain-focused fine-tunes can rival larger models in specific cultural contexts.},
  author={Umbet, S. and Murzakhmetov, S. and Sagyndyk, B. and Yakunin, K. and Akishev, T. and Zubitskii, P.},
  year={2025},
  publisher={ACL - Field Matters},
  preview={field1_animated.gif},
  abbr={ACL - Field Matters},
  selected=true,
}

@book{astro,
  title={Automatized search for hub-filament systems in numerical simulations data},
  author={Makarova, D. and Umbet, S. and Alina, D.},
  year={2023},
  publisher={Cold Cores},
  preview={Mel15_anim2.gif},
  selected=true,
  url={https://indico.icc.ub.edu/event/213/contributions/1695/attachments/728/1468/Makarova_Presentation.pdf},
  html={https://indico.icc.ub.edu/event/213/contributions/1695/attachments/728/1468/Makarova_Presentation.pdf},
  abbr={Astrophysics},
  selected=true,
}